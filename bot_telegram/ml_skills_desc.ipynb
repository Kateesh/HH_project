{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import csv\n",
    "import datetime\n",
    "\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import re\n",
    "\n",
    "\n",
    "def clear_txt(txt):\n",
    "    stemmer = SnowballStemmer('russian')\n",
    "    txt = txt.lower()\n",
    "    #txt = re.sub('[/+_!@#$A-Za-z0-9\\n.,:()\"\"«»;-]', ' ', txt)\n",
    "    txt = re.sub('[/+_!@#$0-9\\n.,:()\"\"«»;-]', ' ', txt)\n",
    "    new_txt = ''\n",
    "    for t in txt.split(' '):\n",
    "        if len(t) > 0:\n",
    "            new_txt = new_txt + stemmer.stem(t) + ' '\n",
    "    return new_txt[:-1]\n",
    "\n",
    "\n",
    "def match_description(df3, exp_len, exp_desc):\n",
    "    \n",
    "    w2v_model = Word2Vec.load(\"w2v_description.model\")\n",
    "    \n",
    "    df3['description_stem'] = df3['description_stem'].fillna('--')\n",
    "    \n",
    "    sentences = list(df3['description_stem'])\n",
    "    new_sentences = [list(i.split(' ')) for i in sentences] \n",
    "    sentences = new_sentences\n",
    "\n",
    "    target_sentence = exp_desc.replace(',', ' ').lower()\n",
    "    target_sentence\n",
    "\n",
    "    # use n_similarity to compute a cosine similarity (should be reasonably robust)\n",
    "    sentences_similarity = np.zeros(len(sentences))\n",
    "    indexes = np.zeros(len(sentences))\n",
    "\n",
    "    target_sentence_words = [w for w in target_sentence.split(' ') if w in w2v_model.wv.index_to_key]\n",
    "\n",
    "    for idx, sentence in enumerate(sentences):\n",
    "        \n",
    "        sentence_words = [w for w in sentence if w in w2v_model.wv.index_to_key]\n",
    "\n",
    "        sim = w2v_model.wv.n_similarity(target_sentence_words, sentence_words)\n",
    "        sentences_similarity[idx] = sim\n",
    "        \n",
    "        ss = ' '.join(sentence)\n",
    "        indexes[idx] = df3[df3['description_stem']==ss].index[0]\n",
    "\n",
    "    #result = list(zip(indexes, sentences_similarity, sentences))\n",
    "    result = list(zip(indexes, sentences_similarity))\n",
    "    result.sort(key=lambda item:item[1], reverse=True)\n",
    "    print(\"Target:\", target_sentence)\n",
    "    #print(result)\n",
    "    \n",
    "    df = pd.DataFrame(zip(indexes, sentences_similarity), columns=['Indexes', 'Desc_Similarity'])\n",
    "    \n",
    "    return (df)\n",
    "\n",
    "\n",
    "def match_skills(df3, exp_len, skills):\n",
    "    \n",
    "    df3['skills'] = df3['skills'].str.replace(',', ' ').str.lower().fillna('--')    \n",
    "    \n",
    "    w2v_model = Word2Vec.load(\"w2v_skills.model\")\n",
    "    \n",
    "    sentences = list(df3['skills'])\n",
    "    new_sentences = [list(i.split(' ')) for i in sentences] \n",
    "    sentences = new_sentences\n",
    "\n",
    "    target_sentence = skills.replace(',', ' ').lower()\n",
    "    target_sentence\n",
    "\n",
    "    sentences_similarity = np.zeros(len(sentences))\n",
    "    indexes = np.zeros(len(sentences))\n",
    "\n",
    "    target_sentence_words = [w for w in target_sentence.split(' ') if w in w2v_model.wv.index_to_key]\n",
    "\n",
    "    for idx, sentence in enumerate(sentences):\n",
    "        sentence_words = [w for w in sentence if w in w2v_model.wv.index_to_key]\n",
    "\n",
    "        sim = w2v_model.wv.n_similarity(target_sentence_words, sentence_words)\n",
    "        sentences_similarity[idx] = sim\n",
    "\n",
    "        ss = ' '.join(sentence)\n",
    "        indexes[idx] = df3[df3['skills']==ss].index[0]\n",
    "\n",
    "    result = list(zip(indexes, sentences_similarity, sentences))\n",
    "    result.sort(key=lambda item:item[1], reverse=True)\n",
    "    print(\"Target:\", target_sentence)\n",
    "    \n",
    "    df = pd.DataFrame(zip(indexes, sentences_similarity), columns=['Indexes', 'Skill_Similarity'])\n",
    "    \n",
    "    return (df)\n",
    "\n",
    "\n",
    "def match_vacancy(person):\n",
    "    \n",
    "    print(datetime.datetime.now())\n",
    "    \n",
    "    df3 = pd.read_csv(os.path.join('input', 'data_processed_add_stems.csv'), encoding = 'utf-16')\n",
    "\n",
    "    df3.loc[df3['experience.id'] == 'between3And6', 'exp_length_hh'] = 3\n",
    "    df3.loc[df3['experience.id'] == 'between1And3', 'exp_length_hh'] = 1\n",
    "    df3.loc[df3['experience.id'] == 'noExperience', 'exp_length_hh'] = 0\n",
    "    df3.loc[df3['experience.id'] == 'moreThan6', 'exp_length_hh'] = 6\n",
    "    \n",
    "    data = person.split(';')\n",
    "    print(data)\n",
    "    skills = data[1]    \n",
    "    exp_len = data[0]\n",
    "    experience = data[2]\n",
    "    \n",
    "    \n",
    "    #s1 = re.sub('[^A-Za-zА-Яа-я]+', ' ', experience)\n",
    "    s1 = re.sub('\\W+',' ', experience )\n",
    "    \n",
    "    exp_processed = clear_txt(s1)\n",
    "    \n",
    "    \n",
    "    print(df3['exp_length_hh'].unique())\n",
    "    \n",
    "    df3 = df3[df3['exp_length_hh']<= (int(exp_len) + 1)]\n",
    "    \n",
    "    # выберем 1000 ваканасий\n",
    "    df3 = df3.sample(n = 5000)\n",
    "\n",
    "    print(len(df3))\n",
    "\n",
    "    x = match_description(df3, exp_len, exp_processed)\n",
    "    y = match_skills(df3, exp_len, skills)\n",
    "    \n",
    "    out_df = x.merge(y, on='Indexes', how='inner')\n",
    "    \n",
    "    out_df['Combined_Similarity'] = out_df['Skill_Similarity'] * 0.6 + out_df['Desc_Similarity'] * 0.4\n",
    "    \n",
    "    ids = out_df.sort_values(by=['Combined_Similarity'], ascending=False)[:5]['Indexes']\n",
    "    \n",
    "    \n",
    "    \n",
    "    '''\n",
    "    ids = []\n",
    "\n",
    "    for x in result[:5]:\n",
    "        ids.append (x[0])\n",
    "\n",
    "    '''\n",
    "\n",
    "    out =''\n",
    "    \n",
    "\n",
    "    for i in ids:\n",
    "        print (df3.loc[i, 'alternate_url'], df3.loc[i, 'name'])\n",
    "        print (df3.loc[i, 'skills'])\n",
    "        print (df3.loc[i, 'requirement'])\n",
    "        print ('\\n')\n",
    "        \n",
    "        out = out + df3.loc[i, 'alternate_url'] + ' ' + df3.loc[i, 'name'] + os.linesep + \\\n",
    "                    df3.loc[i, 'experience.name'] + \\\n",
    "              os.linesep + df3.loc[i, 'requirement'] + os.linesep + os.linesep\n",
    "\n",
    "    \n",
    "    print(datetime.datetime.now())\n",
    "    \n",
    "    \n",
    "    return (out)\n",
    "    #return(ids)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}